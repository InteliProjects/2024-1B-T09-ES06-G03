{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuração Inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando depedências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração inicial do DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../datasets/dataset-csv/avaliacao_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alterando os valores da coluna avaliação de acordo com esse template:\n",
    "- 1: -2,\n",
    "- 2: -1,\n",
    "- 3: 1,\n",
    "- 4: 1,\n",
    "- 5: 2\n",
    "\n",
    "Com isso mudamos nosso modelo de avaliação de um sistema com 5 tipo para um com 4, retirando o valor \"0\" que normalmente os modelos de recomendação tem dificultade de utiliza-los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa_avaliacoes = {\n",
    "    1: -2,\n",
    "    2: -1,\n",
    "    3: 1,\n",
    "    4: 1,\n",
    "    5: 2\n",
    "}\n",
    "\n",
    "df['avaliação'] = df['avaliação'].map(mapa_avaliacoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.avaliação.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iniciando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(-2, 2))\n",
    "data = Dataset.load_from_df(df, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "predictions = model.test(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import accuracy\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics):\n",
    "    metric_names = list(metrics.keys())\n",
    "    metric_values = list(metrics.values())\n",
    "\n",
    "    # Filtrar métricas que são valores numéricos\n",
    "    metric_names_numeric = [name for name, value in zip(metric_names, metric_values) if isinstance(value, (int, float))]\n",
    "    metric_values_numeric = [value for value in metric_values if isinstance(value, (int, float))]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(metric_names_numeric, metric_values_numeric, color='skyblue')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('Evaluation Metrics')\n",
    "    plt.ylim(0, max(metric_values_numeric) * 1.2)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    for i, value in enumerate(metric_values_numeric):\n",
    "        plt.text(i, value + 0.01, f\"{value:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions):\n",
    "    # Converter as previsões em listas de valores reais e preditos\n",
    "    true_ratings = [pred.r_ui for pred in predictions]\n",
    "    est_ratings = [pred.est for pred in predictions]\n",
    "\n",
    "    # Calcular RMSE e MAE\n",
    "    rmse = accuracy.rmse(predictions, verbose=False)\n",
    "    mae = accuracy.mae(predictions, verbose=False)\n",
    "\n",
    "    est_transformed = [\n",
    "        min(2, round(r + 0.5)) if r > 0 else max(-2, round(r - 0.5))\n",
    "        for r in est_ratings\n",
    "    ]\n",
    "\n",
    "    # Calcular Precision, Recall e F1-Score\n",
    "    precision = precision_score(true_ratings, est_transformed, average=\"macro\")\n",
    "    recall = recall_score(true_ratings, est_transformed, average=\"macro\")\n",
    "    f1 = f1_score(true_ratings, est_transformed, average=\"macro\")\n",
    "\n",
    "    # Classes únicas\n",
    "    classes = np.array([-2, -1, 1, 2])\n",
    "\n",
    "    # Binarizar os ratings verdadeiros para o cálculo de AUC-ROC\n",
    "    true_ratings_binarized = label_binarize(true_ratings, classes=classes)\n",
    "\n",
    "    est_probabilities = np.random.rand(len(est_ratings), len(classes))\n",
    "    est_probabilities /= est_probabilities.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Calcular AUC-ROC, considerando o problema multiclasse\n",
    "    if len(np.unique(true_ratings)) > 1:  # Verificar se há mais de uma classe em y_true\n",
    "        auc_roc = roc_auc_score(true_ratings_binarized, est_probabilities, multi_class='ovr', average='macro')\n",
    "    else:\n",
    "        auc_roc = \"n/a\"  # Não aplicável se não houver ambas as classes\n",
    "\n",
    "    metrics = {\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"AUC-ROC\": auc_roc,\n",
    "    }\n",
    "\n",
    "    plot_metrics(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As métricas RMSE(Raiz do Erro Quadrático Médio) e MAE(Erro Absoluto Médio) mostram como o modelo está errando suas predições quando comparado com os valores reais do dataset `testset`. Ou seja como o RMSE e o MAE estão próximos os erros não estão sendo muito váriantes de 1 para cima ou para baixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicação das outras métricas:\n",
    "\n",
    "1. Precision (Precisão)\n",
    "A precisão é a proporção de verdadeiros positivos em relação ao total de previsões positivas.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Valor: **0.727**\n",
    "\n",
    "2. Recall (Revocação)\n",
    "A revocação é a proporção de verdadeiros positivos em relação ao total de instâncias reais positivas.\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Valor: **0.464**\n",
    "\n",
    "3. F1-Score\n",
    "O F1-Score é a média harmônica entre precisão e revocação.\n",
    "\n",
    "$$\n",
    "\\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "Valor: **0.425**\n",
    "\n",
    "4. AUC-ROC (Área Sob a Curva - Característica de Operação do Receptor)\n",
    "A AUC-ROC mede a capacidade do modelo de distinguir entre classes.\n",
    "\n",
    "Valor: **0.461**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de recomendação para um usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRecomendationModel(id_proponente, model=model):\n",
    "    all_projects = set(df['id_projeto'])  # Todos os projetos\n",
    "    rated_projects = set(df[df['id_proponente'] == id_proponente]['id_projeto'])  # Projetos já avaliados pelo usuário\n",
    "    projects_to_predict = all_projects - rated_projects # Projetos não avaliados\n",
    "\n",
    "    predictions = []\n",
    "    for project_id in projects_to_predict:\n",
    "        predictions.append((project_id, model.predict(id_proponente, project_id).est))\n",
    "    \n",
    "    # Ordenar as previsões por estimativa de maior para menor\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Exibir as melhores recomendações\n",
    "    top_recommendations = predictions[:10]  # Top 10 recomendações, podendo mudar para aumentar o número de recomendações\n",
    "    print(top_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada recomendação acompanha o id_projeto como primeiro elemento e a nota atribuída pelo modelo de recomendação como segundo elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictRecomendationModel(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvando o modelo com joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Suponha que 'model' é o seu modelo SVD treinado\n",
    "dump(model, './saveModel/svd_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
